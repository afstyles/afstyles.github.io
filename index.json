[{"authors":["admin"],"categories":null,"content":"I am a PhD student studying physical oceanography at the University of Oxford. I am based at the department of Atmospheric, Oceanic and Planetary Physics (AOPP) and a student of the Environmental Research DTP.\nI take an interest in the fluid dynamics of the ocean circulation. I study the dynamics of ocean gyres and how they are influenced by both mesoscale eddies and interactions with the sea floor. I design idealized models, develop diagnostic methods, and use pen-and-paper analysis to find minimal descriptions of ocean processes seen in models and observations.\nMy current research project is a collaboration between the University of Oxford and the Met Office.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://afstyles.github.io/author/andrew-styles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/andrew-styles/","section":"authors","summary":"I am a PhD student studying physical oceanography at the University of Oxford. I am based at the department of Atmospheric, Oceanic and Planetary Physics (AOPP) and a student of the Environmental Research DTP.","tags":null,"title":"Andrew Styles","type":"authors"},{"authors":["Andrew F. Styles","Michael J. Bell","David P. Marshall","David Storkey"],"categories":null,"content":"#Supplementary notes can be added here, including code, math, and images.\n","date":1653264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653264000,"objectID":"acfb9dbfb17b0afe6b3b9d07719ed186","permalink":"https://afstyles.github.io/publication/spuriousforces/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/publication/spuriousforces/","section":"publication","summary":"Gyres are prominent surface structures in the global ocean circulation that often interact with the sea floor in a complex manner. Diagnostic methods, such as the depth-integrated vorticity budget, are needed to assess exactly how such model circulations interact with the bathymetry. Terms in the vorticity budget can be integrated over the area enclosed by streamlines to identify forces that spin gyres up and down. In this article we diagnose the depth-integrated vorticity budgets of both idealized gyres and the Weddell Gyre in a realistic global model. It is shown that spurious forces play a significant role in the dynamics of all gyres presented and that they are a direct consequence of the Arakawa C-grid discretization and the z-coordinate representation of the sea floor. The spurious forces include a numerical beta effect and interactions with the sea floor which originate from the discrete Coriolis force when calculated with the following schemes: the energy conserving scheme; the enstrophy conserving scheme; and the energy and enstrophy conserving scheme. Previous studies have shown that bottom pressure torques provide the main interaction between the depth-integrated flow and the sea floor. Bottom pressure torques are significant, but spurious interactions with bottom topography are similar in size. Possible methods for reducing the identified spurious topographic forces are discussed. Spurious topographic forces can be alleviated by using either a B-grid in the horizontal plane or a terrain-following vertical coordinate.","tags":[],"title":"Spurious Forces Can Dominate the Vorticity Budget of Ocean Gyres on the C-Grid","type":"publication"},{"authors":null,"categories":null,"content":"This year I will be presenting my work at the EGU 2022 conference in Vienna.\nI will be presenting my work on spurious forces in the Numerical Modelling of the Ocean session (link here).\nMy presentation will be on Wednesday, 25th May at 08:50 CEST in Room 1.15/16.\nSee you in Vienna (or online)!\n","date":1650499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650499200,"objectID":"d6ce4f62d76cc970a672141739351d47","permalink":"https://afstyles.github.io/post/egu-2022-presentation/","publishdate":"2022-04-21T00:00:00Z","relpermalink":"/post/egu-2022-presentation/","section":"post","summary":"This year I will be presenting my work at the EGU 2022 conference in Vienna.\nI will be presenting my work on spurious forces in the Numerical Modelling of the Ocean session (link here).","tags":null,"title":"I will be presenting at EGU 2022","type":"post"},{"authors":null,"categories":null,"content":"The preprint of my first paper is available on ESSOAr.\nIn this article we discuss how the model grid can produce dominant spurious forces in both realistic and idealized model gyres.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"6df94b790d6330dbb7209409589b0261","permalink":"https://afstyles.github.io/post/preprint-post/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/post/preprint-post/","section":"post","summary":"The preprint of my first paper is available on ESSOAr.\nIn this article we discuss how the model grid can produce dominant spurious forces in both realistic and idealized model gyres.","tags":null,"title":"Preprint of first paper","type":"post"},{"authors":null,"categories":null,"content":" Note: This guide has been updated to work with NEMO 4.0.1 using Intel compilers. If for any reason you want to look at my original guide for an earlier release using GCC compilers click here\n These are my personal notes on how to set up and compile NEMO 4.0.1 on Monsoon/NEXCS.\nWhen compiling NEMO I could not find any guide for this particular system but used the following excellent guides for inspiration:\n   Julian Mak\u0026rsquo;s guide to compiling NEMO 4.0 on local machines and Oxford ARC.\n   Christopher Bull\u0026rsquo;s guide for compiling NEMO on ARCHER\n  As these guides were so helpful, I feel it is only fair to make a similar contribution.\nI must also give a huge thanks to Dr. Andrew Coward from the National Oceanography Centre who has helped me to update this guide.\nStep 1 - Loading modules Once you are logged on to the Met Office lander, connect to NEXCS using\nssh xcs-c  On logging in you need to load the correct compilers and libraries. You will not be able to compile XIOS or NEMO without loading the correct modules. You can do this by running the following shell script.\n#/bin/bash #! module swap PrgEnv-cray/5.2.82 PrgEnv-intel/5.2.82 module swap intel/15.0.0.090 intel/18.0.5.274 module load cray-hdf5-parallel/1.10.2.0 module load cray-netcdf-hdf5parallel/4.6.1.3  It will also be useful to load an editor\nmodule load nano   Note: When you log in to NEXCS a set of essential modules are loaded by default. Do not use module purge before loading the above modules.\n Your loaded modules should look like the following:\nastyles@xcslc0:~\u0026gt; module list Currently Loaded Modulefiles: 1) moose-client-wrapper 9) craype-network-aries 17) cray-libsci/13.0.1 25) alps/5.2.4-2.0502.9774.31.11.ari 2) python/v2.7.9 10) gcc/4.8.1 18) udreg/2.3.2-1.0502.10518.2.17.ari 26) rca/1.0.0-2.0502.60530.1.62.ari 3) metoffice/tempdir 11) intel/18.0.5.274 19) ugni/6.0-1.0502.10863.8.29.ari 27) atp/1.7.5 4) metoffice/userenv 12) craype/2.2.1 20) pmi/5.0.5-1.0000.10300.134.8.ari 28) PrgEnv-intel/5.2.82 5) subversion-1.8/1.8.19 13) craype-haswell 21) dmapp/7.0.1-1.0502.11080.8.76.ari 29) cray-hdf5-parallel/1.10.2.0 6) modules/3.2.10.5 14) cray-mpich/7.0.4 22) gni-headers/4.0-1.0502.10859.7.8.ari 30) cray-netcdf-hdf5parallel/4.6.1.3 7) eswrap/1.3.3-1.020200.1280.0 15) pbs/18.2.5.20190913061152 23) xpmem/0.1-2.0502.64982.5.3.ari 8) switch/1.0-1.0502.60522.1.61.ari 16) nano/2.8.6 24) dvs/2.5_0.9.0-1.0502.2188.1.116.ari  Step 2 - XIOS 2.5 XIOS controls the Input/Output for NEMO and needs to be compiled first. For NEMO 4.0 I use XIOS 2.5 but the method is very similar for other versions.\nFirst navigate to the data directory on NEXCS and make a folder for XIOS\ncd $DATADIR mkdir XIOS  Inside the folder download XIOS 2.5\ncd XIOS svn checkout -r 1566 http://forge.ipsl.jussieu.fr/ioserver/svn/XIOS/branchs/xios-2.5 xios-2.5  We need to make sure XIOS is looking in the right place for the modules loaded in Step 1. We copy and rename the following files in the arch folder\ncd xios-2.5/arch/ cp arch-XC30_Cray.env arch-XC30_NEXCS.env cp arch-XC30_Cray.fcm arch-XC30_NEXCS.fcm cp arch-XC30_Cray.path arch-XC30_NEXCS.path  Then edit the files so they look like the following\n#arch-XC30_NEXCS.env export HDF5_INC_DIR=${HDF5_DIR}/include export HDF5_LIB_DIR=${HDF5_DIR}/lib export NETCDF_INC_DIR=${NETCDF_DIR}/include export NETCDF_LIB_DIR=${NETCDF_DIR}/lib   Note: If you are using a different set of modules or worried that this path is wrong. The commands which nc-config and which h5copy will give you a path of the form directory/bin. The paths used above should then be directory/GNU/4.9/include and similar for lib\n #arch-XC30_NEXCS.fcm ################################################################################ ################### Projet XIOS ################### ################################################################################ # Cray XC build instructions for XIOS/xios-1.0 # These files have been tested on # Archer (XC30), ECMWF (XC30), and the Met Office (XC40) using the Cray PrgEnv. # One must also: # module load cray-netcdf-hdf5parallel/4.3.2 # There is a bug in the CC compiler prior to cce/8.3.7 using -O3 or -O2 # The workarounds are not ideal: # Use -Gfast and put up with VERY large executables # Use -O1 and possibly suffer a significant performance loss. # # Mike Rezny Met Office 23/03/2015 %CCOMPILER cc %FCOMPILER ftn %LINKER CC %BASE_CFLAGS -DMPICH_SKIP_MPICXX -h msglevel_4 -h zero -h noparse_templates %PROD_CFLAGS -O3 -DBOOST_DISABLE_ASSERTS %DEV_CFLAGS -g -O2 %DEBUG_CFLAGS -g %BASE_FFLAGS -warn all -zero %PROD_FFLAGS -O3 -fp-model precise -warn all -zero %DEV_FFLAGS -g -O2 %DEBUG_FFLAGS -g %BASE_INC -D__NONE__ %BASE_LD -D__NONE__ %CPP cpp %FPP cpp -P -CC %MAKE gmake   #arch-XC30_NEXCS.path NETCDF_INCDIR=\u0026quot;-I $NETCDF_INC_DIR\u0026quot; NETCDF_LIBDIR='-Wl,\u0026quot;--allow-multiple-definition\u0026quot; -Wl,\u0026quot;-Bstatic\u0026quot; -L $NETCDF_LIB_DIR' NETCDF_LIB=\u0026quot;-lnetcdf -lnetcdff\u0026quot; MPI_INCDIR=\u0026quot;\u0026quot; MPI_LIBDIR=\u0026quot;\u0026quot; MPI_LIB=\u0026quot;\u0026quot; #HDF5_INCDIR=\u0026quot;-I $HDF5_INC_DIR\u0026quot; HDF5_LIBDIR=\u0026quot;-L $HDF5_LIB_DIR\u0026quot; HDF5_LIB=\u0026quot;-lhdf5_hl -lhdf5 -lz\u0026quot; OASIS_INCDIR=\u0026quot;\u0026quot; OASIS_LIBDIR=\u0026quot;\u0026quot; OASIS_LIB=\u0026quot;\u0026quot; #OASIS_INCDIR=\u0026quot;-I$PWD/../../prism/X64/build/lib/psmile.MPI1\u0026quot; #OASIS_LIBDIR=\u0026quot;-L$PWD/../../prism/X64/lib\u0026quot; #OASIS_LIB=\u0026quot;-lpsmile.MPI1 -lmpp_io\u0026quot;  Then you need to edit bld.cfg in xios-2.5/\ncd $DATADIR/XIOS/xios-2.5  and change all occurrences of src_netcdf to src_netcdf4 (two in total)\nYou are now ready to compile XIOS. In xios-2.5/\n./make_xios --full --prod --arch XC30_NEXCS -j2  If this build is successful then XIOS is ready to use. If not, check over the arch files and make sure the flags are correct and in the right order. Also make sure you have the correct modules loaded. Getting XIOS to compile is by far the most difficult part of the process.\nStep 3 - NEMO 4.0 Now we have XIOS ready to go we can start working on NEMO 4.0.\nGo back to the data directory and create a new folder for NEMO\ncd $DATADIR mkdir NEMO  Then download NEMO 4.0.1 in the folder.\ncd NEMO svn co https://forge.ipsl.jussieu.fr/nemo/svn/NEMO/releases/release-4.0.1  Now we need to copy and update the arch files for NEMO like we did in XIOS.\ncd release-4.0.1/arch/ cp arch-linux_ifort.fcm arch-XC_MONSOON_INTEL.fcm  The file arch-XC_MONSOON_INTEL.fcm needs to look like\n#arch-XC_MONSOON_INTEL.fcm %NCDF_HOME $NETCDF_DIR %HDF5_HOME $HDF5_DIR %XIOS_HOME $DATADIR/XIOS/xios-2.5 #OASIS_HOME %NCDF_INC -I%NCDF_HOME/include -I%HDF5_HOME/include %NCDF_LIB -L%HDF5_HOME/lib -L%NCDF_HOME/lib -lnetcdff -lnetcdf -lhdf5_hl -lhdf5 -lz %XIOS_INC -I%XIOS_HOME/inc %XIOS_LIB -L%XIOS_HOME/lib -lxios #OASIS_INC -I%OASIS_HOME/build/lib/mct -I%OASIS_HOME/build/lib/psmile.MPI1 #OASIS_LIB -L%OASIS_HOME/lib -lpsmile.MPI1 -lmct -lmpeu -lscrip %CPP cpp %FC ftn %FCFLAGS -integer-size 32 -real-size 64 -O3 -fp-model source -zero -fpp -warn all %FFLAGS -integer-size 32 -real-size 64 -O3 -fp-model source -zero -fpp -warn all %LD CC -Wl,\u0026quot;--allow-multiple-definition\u0026quot; %FPPFLAGS -P -C -traditional %LDFLAGS %AR ar %ARFLAGS -rvs %MK gmake %USER_INC %XIOS_INC %NCDF_INC %USER_LIB %XIOS_LIB %NCDF_LIB #USER_INC %XIOS_INC %OASIS_INC %NCDF_INC ##USER_LIB %XIOS_LIB %OASIS_LIB %NCDF_LIB %CC cc %CFLAGS -O0  Step 4 - Compiling the GYRE_PISCES configuration Now we need a configuration to compile. In this example I use GYRE_PISCES which is a reference configuration in NEMO as it is a relatively simple configuration. Other reference configurations are listed here and the process will be similar.\nCopy the reference configuration you like to use by doing the following.\ncd $DATADIR/NEMO/release-4.0.1/cfgs mkdir GYRE_testing rsync -arv GYRE_PISCES/* GYRE_testing  Then go into GYRE_testing, rename the cpp_GYRE_PISCES.fcm file.\ncd GYRE_testing mv cpp_GYRE_PISCES.fcm cpp_GYRE_testing.fcm  Then edit the same file and replace key_top with key_nosignedzero.\n In cfgs/ edit the file ref_cfgs.txt and add your configuration to the bottom of the list. For this example the file looks like.\nAGRIF_DEMO OCE ICE NST AMM12 OCE C1D_PAPA OCE GYRE_BFM OCE TOP GYRE_PISCES OCE TOP ORCA2_OFF_PISCES OCE TOP OFF ORCA2_OFF_TRC OCE TOP OFF ORCA2_SAS_ICE OCE ICE NST SAS ORCA2_ICE_PISCES OCE TOP ICE NST SPITZ12 OCE ICE GYRE_testing OCE TOP   Note: Make sure the OCE/TOP/\u0026hellip; flags match the reference configuration you are using\n You are now ready to compile nemo. Make sure you have all the modules in Step 1 loaded. Go back to the base directory and do the following\ncd $DATADIR/NEMO/release-4.0.1 ./makenemo -j2 -r GYRE_testing -m linux_NEXCS  If all has gone well you have successfully compiled NEMO!\nStep 5 - Using NEMO on NEXCS Now that you have compiled NEMO, you probably want to run it.\nNEMO runs in parallel and will output a data file for each thread used. Therefore we need the REBUILD_NEMO tool to recombine the outputs into a single file.\nTo do this go to the tools folder and execute the following command:\ncd $DATADIR/NEMO/nemo4.0-9925/tools ./maketools -n REBUILD_NEMO -m linux_NEXCS   Note: If you get an error about iargc_ and/or getarg not being external variables then go to the following file tools/REBUILD_NEMO/src/rebuild_nemo.F90 and comment out these two lines.\nINTEGER, EXTERNAL :: iargc ... external :: getarg  Depending on the compiler used these may or may not be treated as external variables.\n Now go back to your configuration and create a symbolic link to xios_server.exe\ncd $DATADIR/NEMO/release-4.0.1/cfgs/GYRE_testing/EXP00 ln -s $DATADIR/XIOS/xios-2.5/bin/xios_server.exe .  Also modify iodef.xml and set using_server to true\n\u0026lt;variable id=\u0026quot;using_server\u0026quot; type=\u0026quot;bool\u0026quot;\u0026gt;true\u0026lt;/variable\u0026gt;  Make an OUTPUTS/ and RESTARTS/ directory in EXP00\nmkdir OUTPUTS RESTARTS  You also need to add a shell script for post processing to EXP00. The one I use below is a modified version of Julian Mak\u0026rsquo;s post processing script. It is too long to show on this page but you can find it here and it should work for this example. The only values that need changing are NUM_DOM(how many nodes is NEMO using) and NUM_CPU (how many cpus in total).\nTo run a job on NEXCS you need to use a submission script like the one below. Yet again this is a modification of Julian\u0026rsquo;s submission script.\n#!/bin/bash #! ##subm_script.pbs #PBS -q nexcs #PBS -A user ##Username #PBS -l select=3 ##(Number of nodes running NEMO + Number of nodes running XIOS) #PBS -l walltime=00:10:00 ##Maximum runtime #PBS -N GYRE_testing ##Name of run in queue #PBS -o testing.output ##Name of output file #PBS -e testing.error ##Name of error file #PBS -j oe #PBS -V cd $DATADIR/NEMO/release-4.0.1/cfgs/GYRE_testing/EXP00/ echo \u0026quot; _ __ ___ _ __ ___ ___ \u0026quot; echo \u0026quot;| '_ \\ / _ \\ '_ ' _ \\ / _ \\ \u0026quot; echo \u0026quot;| | | | __/ | | | | | (_) | \u0026quot; echo \u0026quot;|_| |_|\\___|_| |_| |_|\\___/ v4.0 \u0026quot; export OCEANCORES=64 #Number of cores used (32 per node running NEMO) export XIOSCORES=2 #Number of cores to run XIOS (=number of nodes running NEMO) ulimit -c unlimited ulimit -s unlimited aprun -b -n $XIOSCORES -N 2 ./xios_server.exe : -n $OCEANCORES -N 32 ./nemo #=============================================================== # POSTPROCESSING #=============================================================== # kills the daisy chain if there are errors if grep -q 'E R R O R' ocean.output ; then echo \u0026quot;E R R O R found, exiting...\u0026quot; echo \u0026quot; ___ _ __ _ __ ___ _ __ \u0026quot; echo \u0026quot; / _ \\ '__| '__/ _ \\| '__| \u0026quot; echo \u0026quot;| __/ | | | | (_) | | \u0026quot; echo \u0026quot; \\___|_| |_| \\___/|_| \u0026quot; echo \u0026quot;check out ocean.output or stdouterr to see what the deal is \u0026quot; exit else echo \u0026quot;going into postprocessing stage...\u0026quot; # cleans up files, makes restarts, moves files, resubmits this pbs bash ./postprocess.sh \u0026gt;\u0026amp; cleanup.log exit fi  This script uses two nodes (each with 32 cores) to run NEMO and an additional node to run XIOS.\n Note: For the postprocessing to work the number of nodes used for NEMO (not XIOS) must = NUM_DOM and the number of cores for NEMO = NUM_CPU\n Submit this script by doing\nqsub subm_script.pbs  Check the status of the submitted job using qstat\nIf all goes well you should have outputs in the OUTPUTS/ folder. If not, check testing.output and ocean.output to see what the issue is.\n Note: Make sure you have the modules mentioned in Step 1 loaded when submitting\n Note: If you have an error referring to namberg have a look in namelist_refand look for a comment character ! right next to a variable.\n.true.!comment  If found, add a space.\n.true. !comment   Congratulations, you have successfully compiled and run NEMO!\n","date":1603843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603843200,"objectID":"14f3792736476d67e8b0db9a1d97550f","permalink":"https://afstyles.github.io/nemo-compilation/","publishdate":"2020-10-28T00:00:00Z","relpermalink":"/nemo-compilation/","section":"","summary":"A guide to compiling NEMO 4.0 on Monsoon/NEXCS.","tags":null,"title":"Compiling NEMO 4.0.1 on Monsoon/NEXCS","type":"page"},{"authors":null,"categories":null,"content":"Check out my new updated guide here\n","date":1603843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603843200,"objectID":"86c3f2119b67b11243d17fa2cdd2be82","permalink":"https://afstyles.github.io/post/nemo-compilation-post-update-one/","publishdate":"2020-10-28T00:00:00Z","relpermalink":"/post/nemo-compilation-post-update-one/","section":"post","summary":"Check out my new updated guide here","tags":null,"title":"NEMO compilation guide updated","type":"post"},{"authors":null,"categories":null,"content":" Note this guide has been updated here to work with a later release of NEMO using intel compilers. This guide still works but I would advise using the more up-to-date guide.\n These are my personal notes on how to set up and compile NEMO 4.0 on NEXCS.\nNEXCS is the NERC partition of the Met Office Cray XC40 supercomputer and has a similar infrastructure to Monsoon2.\nWhen compiling NEMO I could not find any guide for this particular system but used the following excellent guides for inspiration:\n   Julian Mak\u0026rsquo;s guide to compiling NEMO 4.0 on local machines and Oxford ARC.\n   Christopher Bull\u0026rsquo;s guide for compiling NEMO on ARCHER\n  As these guides were so helpful, I feel it is only fair to make a similar contribution.\nStep 1 - Loading modules Once you are logged on to the Met Office lander, connect to NEXCS using\nssh xcs-c  On logging in you need to load the correct compilers and libraries. You will not be able to compile XIOS or NEMO without loading the correct modules. First load the netcdf and hdf5 libraries.\nmodule load cray-netcdf-hdf5parallel/4.4.1 module load cray-hdf5-parallel/1.10.0  Switch to a more recent version of mpich.\nmodule swap cray-mpich/7.0.4 cray-mpich/7.3.1  Finally switch from the Cray environment to the GNU environment\nmodule swap PrgEnv-cray/5.2.82 PrgEnv-gnu/5.2.82  It will also be useful to load an editor\nmodule load nano   Note: When you log in to NEXCS a set of essential modules are loaded by default. Do not use module purge before loading the above modules.\n You loaded modules should look like the following:\nuser@xcslc0:~\u0026gt; module list 1) moose-client-wrapper 9) craype-network-aries 17) cray-hdf5-parallel/1.10.0 25) dvs/2.5_0.9.0-1.0502.2188.1.116.ari 2) python/v2.7.9 10) gcc/4.9.1 18) cray-libsci/13.0.1 26) alps/5.2.4-2.0502.9774.31.11.ari 3) metoffice/tempdir 11) craype/2.2.1 19) udreg/2.3.2-1.0502.10518.2.17.ari 27) rca/1.0.0-2.0502.60530.1.62.ari 4) metoffice/userenv 12) craype-haswell 20) ugni/6.0-1.0502.10863.8.29.ari 28) atp/1.7.5 5) subversion-1.8/1.8.19 13) cray-mpich/7.3.1 21) pmi/5.0.5-1.0000.10300.134.8.ari 29) PrgEnv-gnu/5.2.82 6) modules/3.2.10.5 14) pbs/18.2.5.20190913061152 22) dmapp/7.0.1-1.0502.11080.8.76.ari 7) eswrap/1.3.3-1.020200.1280.0 15) nano/2.8.6 23) gni-headers/4.0-1.0502.10859.7.8.ari 8) switch/1.0-1.0502.60522.1.61.ari 16) cray-netcdf-hdf5parallel/4.4.1 24) xpmem/0.1-2.0502.64982.5.3.ari  Step 2 - XIOS 2.5 XIOS controls the Input/Output for NEMO and needs to be compiled first. For NEMO 4.0 I use XIOS 2.5 but the method is very similar for other versions.\nFirst navigate to the data directory on NEXCS and make a folder for XIOS\ncd $DATADIR mkdir XIOS  Inside the folder download XIOS 2.5\ncd XIOS svn checkout -r 1566 http://forge.ipsl.jussieu.fr/ioserver/svn/XIOS/branchs/xios-2.5 xios-2.5  We need to make sure XIOS is looking in the right place for the modules loaded in Step 1. We copy and rename the following files in the arch folder\ncd xios-2.5/arch/ cp arch-GCC_LINUX.env arch-GCC_NEXCS.env cp arch-GCC_LINUX.fcm arch-GCC_NEXCS.fcm cp arch-GCC_LINUX.path arch-GCC_NEXCS.path  Then edit the files so they look like the following\n#arch-GCC_NEXCS.env export HDF5_INC_DIR=/opt/cray/hdf5/1.10.0/GNU/4.9/include export HDF5_LIB_DIR=/opt/cray/hdf5/1.10.0/GNU/4.9/lib export NETCDF_INC_DIR=/opt/cray/netcdf/4.4.1/GNU/4.9/include export NETCDF_LIB_DIR=/opt/cray/netcdf/4.4.1/GNU/4.9/lib   Note: If you are using a different set of modules or worried that this path is wrong. The commands which nc-config and which h5copy will give you a path of the form directory/bin. The paths used above should then be directory/GNU/4.9/include and similar for lib\n #arch-GCC_NEXCS.fcm ################################################################################ ################### Projet XIOS ################### ################################################################################ %CCOMPILER cc %FCOMPILER ftn %LINKER CC %BASE_CFLAGS -ansi -w %PROD_CFLAGS -O3 -DBOOST_DISABLE_ASSERTS %DEV_CFLAGS -g -O2 %DEBUG_CFLAGS -g %BASE_FFLAGS -D__NONE__ %PROD_FFLAGS -O3 %DEV_FFLAGS -g -O2 %DEBUG_FFLAGS -g %BASE_INC -D__NONE__ %BASE_LD -lstdc++ %CPP cpp %FPP cpp -P %MAKE gmake   #arch-GCC_NEXCS.path NETCDF_INCDIR=\u0026quot;-I $NETCDF_INC_DIR\u0026quot; NETCDF_LIBDIR=\u0026quot;-Wl,'--allow-multiple-definition' -L$NETCDF_LIB_DIR\u0026quot; NETCDF_LIB=\u0026quot;-lnetcdff -lnetcdf\u0026quot; MPI_INCDIR=\u0026quot;\u0026quot; MPI_LIBDIR=\u0026quot;\u0026quot; MPI_LIB=\u0026quot;\u0026quot; HDF5_INCDIR=\u0026quot;-I $HDF5_INC_DIR\u0026quot; HDF5_LIBDIR=\u0026quot;-L $HDF5_LIB_DIR\u0026quot; HDF5_LIB=\u0026quot;-lhdf5_hl -lhdf5 -lhdf5 -lz\u0026quot;   Then you need to edit bld.cfg in xios-2.5/\ncd $DATADIR/XIOS/xios-2.5  and change all occurrences of src_netcdf to src_netcdf4 (two in total)\nYou are now ready to compile XIOS. In xios-2.5/\n./make_xios --full --prod --arch GCC_NEXCS -j2  If this build is successful then XIOS is ready to use. If not, check over the arch files and make sure the flags are correct and in the right order. Also make sure you have the correct modules loaded. Getting XIOS to compile is by far the most difficult part of the process.\nStep 3 - NEMO 4.0 Now we have XIOS ready to go we can start working on NEMO 4.0.\nGo back to the data directory and create a new folder for NEMO\ncd $DATADIR mkdir NEMO  Then download NEMO 4.0 in the folder.\ncd NEMO svn checkout -r 9925 http://forge.ipsl.jussieu.fr/nemo/svn/NEMO/trunk nemo4.0-9925  Now we need to copy and update the arch files for NEMO like we did in XIOS.\ncd nemo4.0-9925/arch/ cp arch-linux_ifort.fcm arch-linux_NEXCS.fcm  The file arch-linux_NEXCS.fcm needs to look like\n#arch-linux_NEXCS.fcm %NCDF_HOME /opt/cray/netcdf/4.4.1/GNU/4.9 %HDF5_HOME /opt/cray/hdf5/1.10.0/GNU/4.9 %XIOS_HOME /projects/nexcs-n02/user/XIOS/xios-2.5 %NCDF_INC -I%NCDF_HOME/include -I%HDF5_HOME/include %NCDF_LIB -L%NCDF_HOME/lib -lnetcdf -lnetcdff -lstdc++ %XIOS_INC -I%XIOS_HOME/inc %XIOS_LIB -L%XIOS_HOME/lib -lxios -lstdc++ %CPP cpp %FC ftn %FCFLAGS -fdefault-real-8 -O3 -funroll-all-loops -fcray-pointer -cpp -ffree-line-length-none %FFLAGS %FCFLAGS %LD %FC %FPPFLAGS -P -C -traditional %LDFLAGS %AR ar %ARFLAGS -rs %MK make %USER_INC %XIOS_INC %NCDF_INC %USER_LIB %XIOS_LIB %NCDF_LIB %CC cc  Step 4 - Compiling the GYRE_PISCES configuration Now we need a configuration to compile. In this example I use GYRE_PISCES which is a reference configuration in NEMO as it is a relatively simple configuration. Other reference configurations are listed here and the process will be similar.\nCopy the reference configuration you like to use by doing the following.\ncd $DATADIR/NEMO/nemo4.0-9925/cfgs mkdir GYRE_testing rsync -arv GYRE_PISCES/* GYRE_testing  Then go into GYRE_testing, rename the cpp_GYRE_PISCES.fcm file.\ncd GYRE_testing mv cpp_GYRE_PISCES.fcm cpp_GYRE_testing.fcm  Then edit the same file and replace key_top with key_nosignedzero.\n In cfgs/ edit the file ref_cfgs.txt and add your configuration to the bottom of the list. For this example the file looks like.\nAGRIF_DEMO OCE ICE NST AMM12 OCE C1D_PAPA OCE GYRE_BFM OCE TOP GYRE_PISCES OCE TOP ORCA2_OFF_PISCES OCE TOP OFF ORCA2_OFF_TRC OCE TOP OFF ORCA2_SAS_ICE OCE ICE NST SAS ORCA2_ICE_PISCES OCE TOP ICE NST SPITZ12 OCE ICE GYRE_testing OCE TOP   Note: Make sure the OCE/TOP/\u0026hellip; flags match the reference configuration you are using\n You are now ready to compile nemo. Make sure you have all the modules in Step 1 loaded. Go back to the base directory and do the following\ncd $DATADIR/NEMO/nemo4.0-9925 ./makenemo -j2 -r GYRE_testing -m linux_NEXCS  If all has gone well you have successfully compiled NEMO!\nStep 5 - Using NEMO on NEXCS Now that you have compiled NEMO, you probably want to run it.\nNEMO runs in parallel and will output a data file for each thread used. Therefore we need the REBUILD_NEMO tool to recombine the outputs into a single file.\nTo do this go to the tools folder and execute the following command:\ncd $DATADIR/NEMO/nemo4.0-9925/tools ./maketools -n REBUILD_NEMO -m linux_NEXCS   Note: If you get an error about iargc_ and/or getarg not being external variables then go to the following file tools/REBUILD_NEMO/src/rebuild_nemo.F90 and comment out these two lines\nINTEGER, EXTERNAL :: iargc ... external :: getarg  These are not external variables in Fortran 90.\n Now go back to your configuration and create a symbolic link to xios_server.exe\ncd $DATADIR/NEMO/nemo4.0-9925/cfgs/GYRE_testing/EXP00 ln -s $DATADIR/XIOS/xios-2.5/bin/xios_server.exe .  Also modify iodef.xml and set using_server to true\n\u0026lt;variable id=\u0026quot;using_server\u0026quot; type=\u0026quot;bool\u0026quot;\u0026gt;true\u0026lt;/variable\u0026gt;  Make an OUTPUTS/ and RESTARTS/ directory in EXP00\nmkdir OUTPUTS RESTARTS  You also need to add a shell script for post processing to EXP00. The one I use below is a modified version of Julian Mak\u0026rsquo;s post processing script. It is too long to show on this page but you can find it here and it should work for this example. The only values that need changing are NUM_DOM(how many nodes is NEMO using) and NUM_CPU (how many cpus in total).\nTo run a job on NEXCS you need to use a submission script like the one below. Yet again this is a modification of Julian\u0026rsquo;s submission script.\n#!/bin/bash #! ##subm_script.pbs #PBS -q nexcs #PBS -A user ##Username #PBS -l select=3 ##(Number of nodes running NEMO + Number of nodes running XIOS) #PBS -l walltime=00:10:00 ##Maximum runtime #PBS -N GYRE_testing ##Name of run in queue #PBS -o testing.output ##Name of output file #PBS -e testing.error ##Name of error file #PBS -j oe #PBS -V cd $DATADIR/NEMO/nemo4.0-9925/cfgs/GYRE_testing/EXP00/ echo \u0026quot; _ __ ___ _ __ ___ ___ \u0026quot; echo \u0026quot;| '_ \\ / _ \\ '_ ' _ \\ / _ \\ \u0026quot; echo \u0026quot;| | | | __/ | | | | | (_) | \u0026quot; echo \u0026quot;|_| |_|\\___|_| |_| |_|\\___/ v4.0 \u0026quot; export OCEANCORES=64 #Number of cores used (32 per node running NEMO) export XIOSCORES=2 #Number of cores to run XIOS (=number of nodes running NEMO) ulimit -c unlimited ulimit -s unlimited aprun -b -n $XIOSCORES -N 2 ./xios_server.exe : -n $OCEANCORES -N 32 ./nemo #=============================================================== # POSTPROCESSING #=============================================================== # kills the daisy chain if there are errors if grep -q 'E R R O R' ocean.output ; then echo \u0026quot;E R R O R found, exiting...\u0026quot; echo \u0026quot; ___ _ __ _ __ ___ _ __ \u0026quot; echo \u0026quot; / _ \\ '__| '__/ _ \\| '__| \u0026quot; echo \u0026quot;| __/ | | | | (_) | | \u0026quot; echo \u0026quot; \\___|_| |_| \\___/|_| \u0026quot; echo \u0026quot;check out ocean.output or stdouterr to see what the deal is \u0026quot; exit else echo \u0026quot;going into postprocessing stage...\u0026quot; # cleans up files, makes restarts, moves files, resubmits this pbs bash ./postprocess.sh \u0026gt;\u0026amp; cleanup.log exit fi  This script uses two nodes (each with 32 cores) to run NEMO and an additional node to run XIOS.\n Note: For the postprocessing to work the number of nodes used for NEMO (not XIOS) must = NUM_DOM and the number of cores for NEMO = NUM_CPU\n Submit this script by doing\nqsub subm_script.pbs  Check the status of the submitted job using qstat\nIf all goes well you should have outputs in the OUTPUTS/ folder. If not, check testing.output and ocean.output to see what the issue is.\n Note: Make sure you have the modules mentioned in Step 1 loaded when submitting\n Note: If you have an error referring to namberg have a look in namelist_refand look for a comment character ! right next to a variable.\n.true.!comment  If found, add a space.\n.true. !comment   Congratulations, you have successfully compiled and run NEMO!\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"0663ab97dcc954c87f577e3048e77e7c","permalink":"https://afstyles.github.io/nemo-compilation-archive/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/nemo-compilation-archive/","section":"","summary":"A guide to compiling NEMO 4.0 on NEXCS.","tags":null,"title":"Compiling NEMO 4.0 on NEXCS","type":"page"},{"authors":null,"categories":null,"content":"Check out my guide to compile NEMO on NEXCS here\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"001289cce087a97836d670b4f0675c4f","permalink":"https://afstyles.github.io/post/nemo-compilation-post/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/post/nemo-compilation-post/","section":"post","summary":"Check out my guide to compile NEMO on NEXCS here","tags":null,"title":"Compiling NEMO on NEXCS","type":"post"}]